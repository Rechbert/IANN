{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e128bd90-4ccc-4c30-acd3-10e0c74f28a6",
   "metadata": {},
   "source": [
    "# Homework 02 - Implementing and Training an MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540bca8f-8078-47a2-887a-e9c87443963b",
   "metadata": {},
   "source": [
    "### 1. Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81f3485-da6f-4265-b295-7d619972e3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4067c05-ad18-4a08-984f-1714901f9154",
   "metadata": {},
   "source": [
    "### 2. MNIST classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c0f457-aec8-4b91-ae96-a087a7f4faa3",
   "metadata": {},
   "source": [
    "#### 2.1. Loading the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057210c3-3ccc-41e2-9e3c-651239883497",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")\n",
    "\n",
    "tfds.show_examples(ds_train, ds_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*How many training/test images are there?*\n",
    "70,000\n",
    "\n",
    "*What's the image shape?*\n",
    "Square images\n",
    "\n",
    "*What range are pixel values in?*\n",
    "8-bit images, so 0-255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd121d09-b34a-4eef-973c-82e47538e13b",
   "metadata": {},
   "source": [
    "#### 2.2. Setting up the data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719598cc-7820-428e-a74a-bbe1c686c739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(mnist):\n",
    "    # flatten images into vectors\n",
    "    mnist = mnist.map(lambda img, target: (tf.reshape(img, (-1,)), target), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    # normalize images from uint8 -> float32\n",
    "    mnist = mnist.map(lambda img, target: (tf.cast(img, tf.float32), target), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    # normalize inputs from 0-255 to [-1, 1]\n",
    "    mnist = mnist.map(lambda img, target: ((img/128.)-1., target), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    # create one-hot targets\n",
    "    mnist = mnist.map(lambda img, target: (img, tf.one_hot(target, depth=10)), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    # cache in memory\n",
    "    mnist = mnist.cache()\n",
    "    mnist = mnist.shuffle(1000)\n",
    "    # mnist = mnist.shuffle(ds_info.splits['train'].num_examples)\n",
    "    mnist = mnist.batch(32)\n",
    "    mnist = mnist.prefetch(20)\n",
    "    # mnist = mnist.prefetch(tf.data.AUTOTUNE)\n",
    "    return mnist\n",
    "\n",
    "ds_train = ds_train.apply(prepare_data)\n",
    "ds_test = ds_test.apply(prepare_data)\n",
    "\n",
    "# ### alternative solution ###\n",
    "# def normalize_img(image, label):\n",
    "#     \"\"\"Normalizes images: uint8 -> float32\"\"\"\n",
    "#     return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "# # data/training pipeline\n",
    "# ds_train = ds_train.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# ds_train = ds_train.cache()\n",
    "# ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "# ds_train = ds_train.batch(128)\n",
    "# ds_train = ds_train.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# # evaluation pipeline\n",
    "# ds_test = ds_test.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# ds_test = ds_test.batch(128)\n",
    "# ds_test = ds_test.cache()\n",
    "# ds_test = ds_test.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3586d40e-8bc2-4b0d-af22-88d99228f6fd",
   "metadata": {},
   "source": [
    "#### 2.3. Building a deep neural network with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f7789a-0aa9-4eac-b6d5-362fcd5d07cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self.__init__())\n",
    "        self.dense1 = tf.keras.layers.Dense(256, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(256, activation='relu')\n",
    "        self.out = tf.keras.layers.Dense(10, activation='softmax')\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "### alternative solution ###\n",
    "# basically the same, only less customizable\n",
    "# model = tf.keras.models.Sequential([\n",
    "#     tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "#     # tf.keras.layers.Dense(256, activation='relu', name='dense1'),\n",
    "#     tf.keras.layers.Dense(128, activation='relu', name='dense2'),\n",
    "#     tf.keras.layers.Dense(10, name='out')\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e3b212-6801-4bfa-9272-e579f533a4f9",
   "metadata": {},
   "source": [
    "#### 2.4. Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, input, target, loss_function, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        prediction = model(input)\n",
    "        loss = loss_function(target, prediction)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "def test(mode, test_data, loss_function):\n",
    "    test_accuracy_aggregator = []\n",
    "    test_loss_aggregator = []\n",
    "\n",
    "    for (input, target) in test_data:\n",
    "        prediction = model(input)\n",
    "        sample_test_loss = loss_function(target, prediction)\n",
    "        sample_test_accuracy = np.argmax(target, axis=1) == np.argmax(prediction, axis=1)\n",
    "        sample_test_accuracy = np.mean(sample_test_accuracy)\n",
    "        test_loss_aggregator.append(sample_test_loss.numpy())\n",
    "        test_accuracy_aggregator.append(np.mean(sample_test_accuracy))\n",
    "    \n",
    "    test_loss = tf.reduce_mean(test_loss_aggregator)\n",
    "    test_accuracy = tf.reduce_mean(test_accuracy_aggregator)\n",
    "\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ac3a4b-3d99-4c0d-8e65-d26d072746f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "### hyperparams\n",
    "num_epochs = 10\n",
    "learning_rate = 0.1\n",
    "\n",
    "model = MyModel()\n",
    "\n",
    "cross_entropy_loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "test_loss, test_accuracy = test(model, test_dataset, cross_entropy_loss)\n",
    "test_losses.append(test_loss)\n",
    "test_accuracies.append(train_loss)\n",
    "\n",
    "# check model performance on training data\n",
    "train_loss, _ = test(model, train_dataset, cross_entropy_loss)\n",
    "train_losses.append(train_loss)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch: {str(epoch)} starting with accuracy {test_accuracies[-1]}')\n",
    "\n",
    "    epoch_loss_agg = []\n",
    "    for input, target in train_dataset:\n",
    "        train_loss = train_step(model, input, cross_entropy_loss, optimizer)\n",
    "        epoch_loss_agg.append(train_loss)\n",
    "    \n",
    "    # track training loss\n",
    "    train_losses.append(tf.reduce_mean(epoch_loss_agg))\n",
    "\n",
    "    # testing to enable accuracy and test loss tracking\n",
    "    test_loss, test_accuracy = test(model, test_dataset, cross_entropy_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "# ### alternative solution ###\n",
    "# model.compile(\n",
    "#     # optimizer=tf.keras.optimizers.SGD(learning_rate),\n",
    "#     optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "#     loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#     metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    "# )\n",
    "\n",
    "# print(\"Fit model on training data\")\n",
    "# model.fit(\n",
    "#     ds_train,\n",
    "#     epochs=num_epochs,\n",
    "#     validation_data=ds_test,\n",
    "# )\n",
    "\n",
    "# results = model.evaluate(ds_train, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1b491c-b6d2-479f-a654-e5600428d9cd",
   "metadata": {},
   "source": [
    "#### 2.5. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557f11a1-6e66-467e-b657-9363e216d254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualization(train_losses, train_accuracies, test_losses, test_accuracies):\n",
    "    \"\"\"\n",
    "    Visualizes accuracy and loss for training and test data using\n",
    "    the mean of each epoch.\n",
    "    Loss is displayed in a regular line, accuracy in a dotted line.\n",
    "    Training data is displayed in blue, test data in red.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train_losses: numpy.ndarray\n",
    "    training losses\n",
    "    train_accuracies: numpy.ndarray\n",
    "    training accuracies\n",
    "    test_losses: numpy.ndarray\n",
    "    test losses\n",
    "    test_accuracies: numpy.ndarray\n",
    "    test accuracies\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure ()\n",
    "    line1, = plt.plot (train_losses, \"b-\")\n",
    "    line2, = plt.plot (test_losses, \"r-\")\n",
    "    line3, = plt.plot (train_accuracies, \"b:\")\n",
    "    line4, = plt.plot (test_accuracies, \"r:\")\n",
    "    plt.xlabel (\"Training steps\")\n",
    "    plt.ylabel (\"Loss / Accuracy\")\n",
    "    plt.legend ((line1, line2, line3, line4), (\"training loss\", \"test loss\", \"train accuracy\", \"test accuracy\"))\n",
    "    plt.show()\n",
    "\n",
    "visualization(train_losses, train_accuracies, test_losses, test_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f6f988-5922-49fb-845e-f3bdd17ba3a2",
   "metadata": {},
   "source": [
    "### 3. Adjusting the hyperparameters of your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tbd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
